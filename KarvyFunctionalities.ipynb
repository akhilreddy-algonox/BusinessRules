{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HOST_IP'] = '18.207.230.116'\n",
    "os.environ['LOCAL_DB_USER'] = 'root'\n",
    "os.environ['LOCAL_DB_PASSWORD'] = 'AlgoTeam123'\n",
    "os.environ['LOCAL_DB_PORT'] = '3306'\n",
    "\n",
    "tenant_id = 'test'\n",
    "\n",
    "db_config = {\n",
    "    'host': os.environ['HOST_IP'],\n",
    "    'user': os.environ['LOCAL_DB_USER'],\n",
    "    'password': os.environ['LOCAL_DB_PASSWORD'],\n",
    "    'port': os.environ['LOCAL_DB_PORT']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load apply_business_rule.py\n",
    "# comment below two for local testing\n",
    "# from ace_logger import Logging\n",
    "# logging = Logging()\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "\n",
    "# uncomment these below lines for local testing\n",
    "import logging \n",
    "logger=logging.getLogger() \n",
    "logger.setLevel(logging.DEBUG) \n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from db_utils import DB \n",
    "\n",
    "from BusinessRules import BusinessRules\n",
    "\n",
    "# one configuration\n",
    "db_config = {\n",
    "    'host': os.environ['HOST_IP'],\n",
    "    'user': os.environ['LOCAL_DB_USER'],\n",
    "    'password': os.environ['LOCAL_DB_PASSWORD'],\n",
    "    'port': os.environ['LOCAL_DB_PORT']\n",
    "}\n",
    "\n",
    "\n",
    "# give any path...platform independent...get the base name\n",
    "def path_leaf(path):\n",
    "    \"\"\"give any path...platform independent...get the base name\"\"\"\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "\n",
    "def to_DT_data(parameters):\n",
    "    \"\"\"Amith's processing for parameters\"\"\"\n",
    "    output = []\n",
    "    try:\n",
    "        for param_dict in parameters:\n",
    "            print(param_dict)    \n",
    "            if param_dict['column'] == 'Add_on_Table':\n",
    "                output.append({'table': param_dict['table'],'column': param_dict['column'],'value': param_dict['value']})\n",
    "                # Need to add a function to show this or tell Kamal check if its addon table and parse accordingly\n",
    "            else:                \n",
    "                output.append({'table': param_dict['table'],'column': param_dict['column'],'value': param_dict['value']})\n",
    "    except:\n",
    "        print(\"Error in to_DT_data()\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "    try:\n",
    "        output = [dict(t) for t in {tuple(d.items()) for d in output}]\n",
    "    except:\n",
    "        print(\"Error in removing duplicate dictionaries in list\")\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "    return output\n",
    "\n",
    "def get_data_sources(tenant_id, case_id, column_name, master=False):\n",
    "    \"\"\"Helper to get all the required table data for the businesss rules to apply\n",
    "    \"\"\"\n",
    "    get_datasources_query = \"SELECT * from `data_sources`\"\n",
    "    business_rules_db = DB('business_rules', tenant_id=tenant_id, **db_config)\n",
    "    data_sources = business_rules_db.execute(get_datasources_query)\n",
    "\n",
    "\n",
    "    # sources\n",
    "    sources = json.loads(list(data_sources[column_name])[0])\n",
    "    \n",
    "    \n",
    "    data = {}\n",
    "    for database, tables in sources.items():\n",
    "        db = DB(database, tenant_id=tenant_id, **db_config)\n",
    "        for table in tables:\n",
    "            if master:\n",
    "                query = f\"SELECT * from `{table}`\"\n",
    "                df = db.execute(query)\n",
    "            else:\n",
    "                query = f\"SELECT * from `{table}` WHERE case_id = %s\"\n",
    "                params = [case_id]\n",
    "                df = db.execute(query, params=params)\n",
    "            if not df.empty:\n",
    "                data[table] = df.to_dict(orient='records')[0]\n",
    "            else:\n",
    "                data[table] = {}\n",
    "    \n",
    "    \n",
    "    case_id_based_sources = json.loads(list(data_sources['case_id_based'])[0])\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def get_rules(tenant_id, group):\n",
    "    \"\"\"Get the rules based on the stage, tenant_id\"\"\"\n",
    "    business_rules_db = DB('business_rules', tenant_id=tenant_id, **db_config)\n",
    "    get_rules_query = \"SELECT * from `sequence_rule_data` where `group` = %s\"\n",
    "    params = [group]\n",
    "    rules = business_rules_db.execute(get_rules_query, params=params)\n",
    "    return rules\n",
    "\n",
    "def update_tables(case_id, tenant_id, updates):\n",
    "    \"\"\"Update the values in the database\"\"\"\n",
    "    try:\n",
    "        extraction_db = DB('extraction', tenant_id=tenant_id, **db_config) # only in ocr or process_queue we are updating\n",
    "        queue_db = DB('queues', tenant_id=tenant_id, **db_config) # only in ocr or process_queue we are updating\n",
    "        \n",
    "        for table, colum_values in updates.items():\n",
    "            if table == 'ocr':\n",
    "                extraction_db.update(table, update=colum_values, where={'case_id':case_id})\n",
    "            if table == 'process_queue':\n",
    "                queue_db.update(table, update=colum_values, where={'case_id':case_id})\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error in updating the tables\")\n",
    "        logging.error(\"check whether the table exists\")\n",
    "        logging.error(e)\n",
    "        \n",
    "    return \"UPDATED IN THE DATABASE SUCCESSFULLY\"\n",
    "\n",
    "\n",
    "def writeToCsv(df, file_path, required_standard_mapping=None):\n",
    "    \"\"\"Write the dataframe to csv\"\"\"\n",
    "    if not required_standard_mapping:\n",
    "        required_standard_mapping = {ele:ele for ele in list(df.columns)}\n",
    "    required_columns = required_standard_mapping.values()\n",
    "    try:\n",
    "\n",
    "        # first find whether any csv exists with the file_name\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        mode = 'a'\n",
    "        headers = False\n",
    "        df.to_csv(file_path, columns=required_columns, mode=mode, header=headers, index=False)\n",
    "    except:\n",
    "        # no sample file exists\n",
    "        headers = True\n",
    "        mode = 'w'\n",
    "        \n",
    "        # rename the columns in the dataframe to standardized columns            \n",
    "        df = df.rename(columns=required_standard_mapping)\n",
    "        df.to_csv(file_path, columns=required_columns, mode=mode, header=headers, index=False)\n",
    "                \n",
    "    return \"Written to csv successfully\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# as of now run this...you can combine the run_chained_rules and column_chained rules with small changes\n",
    "def run_chained_rules_column(file_path, chain_rules, start_rule_id=None):\n",
    "    \"\"\"Execute the chained rules column wise\"\"\"\n",
    "    \n",
    "    # get the mapping of the rules...basically a rule_id maps to a rule\n",
    "    rule_id_mapping = {}\n",
    "    for ind, rule in chain_rules.iterrows():\n",
    "        rule_id_mapping[rule['rule_id']] = [rule['rule_string'], rule['next_if_sucess'], rule['next_if_failure'], rule['stage'], rule['description'], rule['data_source']]\n",
    "    logging.info(f\"\\n rule id mapping is \\n{rule_id_mapping}\\n\")\n",
    "    \n",
    "    # evaluate the rules one by one as chained\n",
    "    # start_rule_id = None\n",
    "    if start_rule_id is None:\n",
    "        if rule_id_mapping.keys():\n",
    "            start_rule_id = list(rule_id_mapping.keys())[0]\n",
    "    \n",
    "    BR  = BusinessRules(None, [], {})\n",
    "    file_name = path_leaf(file_path)[:-4] # stripping the .csv\n",
    "    BR.data_source[file_name] = pd.read_csv(file_path)\n",
    "    \n",
    "    logging.info(f\"\\nStart rule id got is {start_rule_id}\\n \")\n",
    "    while start_rule_id != \"END\":\n",
    "        # get the rules, next rule id to be evaluated\n",
    "        rule_to_evaluate, next_if_sucess, next_if_failure, stage, description, data_source = rule_id_mapping[str(start_rule_id)]  \n",
    "    \n",
    "        logging.info(f\"\\nInside the loop \\n rule_to_evaluate  {rule_to_evaluate}\\n \\\n",
    "                      \\nnext_if_sucess {next_if_sucess}\\n \\\n",
    "                      \\nnext_if_failure {next_if_failure}\\n \")\n",
    "        \n",
    "        # evaluate the rule\n",
    "        BR.rules = [json.loads(rule_to_evaluate)] \n",
    "        BR.evaluate_business_rules() # apply the business rules\n",
    "        start_rule_id = next_if_sucess # no matter what go with next if success\n",
    "        logging.info(f\"\\n next rule id to execute is {start_rule_id}\\n\")\n",
    "        \n",
    "    logging.info(\"\\n Applied chained rules successfully\")\n",
    "    # finally write to the csv\n",
    "    writeToCsv(BR.data_source['file_name'])\n",
    "    return BR\n",
    "\n",
    "\n",
    "\n",
    "def run_chained_rules(case_id, tenant_id, chain_rules, start_rule_id=None, updated_tables=False, trace_exec=None, rule_params=None):\n",
    "    \"\"\"Execute the chained rules\"\"\"\n",
    "    \n",
    "    # get the mapping of the rules...basically a rule_id maps to a rule\n",
    "    rule_id_mapping = {}\n",
    "    for ind, rule in chain_rules.iterrows():\n",
    "        rule_id_mapping[rule['rule_id']] = [rule['rule_string'], rule['next_if_sucess'], rule['next_if_failure'], rule['stage'], rule['description'], rule['data_source']]\n",
    "    logging.info(f\"\\n rule id mapping is \\n{rule_id_mapping}\\n\")\n",
    "    \n",
    "    # evaluate the rules one by one as chained\n",
    "    # start_rule_id = None\n",
    "    if start_rule_id is None:\n",
    "        if rule_id_mapping.keys():\n",
    "            start_rule_id = list(rule_id_mapping.keys())[0]\n",
    "            trace_exec = []\n",
    "            rule_params = {}\n",
    "            \n",
    "    # if start_rule_id then. coming from other service \n",
    "    # get the existing trace and rule params data\n",
    "    business_rules_db = DB('business_rules', tenant_id=tenant_id, **db_config)\n",
    "    rule_data_query = \"SELECT * from `rule_data` where `case_id`=%s\"\n",
    "    params = [case_id]\n",
    "    df = business_rules_db.execute(rule_data_query, params=params)\n",
    "    try:\n",
    "        trace_exec = json.loads(list(df['trace_data'])[0])\n",
    "        logging.info(f\"\\nexistig trace exec is \\n{trace_exec}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"no existing trace data\")\n",
    "        logging.info(f\"{str(e)}\")\n",
    "        trace_exec = []\n",
    "    \n",
    "    try:\n",
    "        rule_params = json.loads(list(df['rule_params'])[0])\n",
    "        logging.info(f\"\\nexistig rule_params is \\n{rule_params}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.info(f\"no existing rule params data\")\n",
    "        logging.info(f\"{str(e)}\")\n",
    "        rule_params = {}\n",
    "       \n",
    "    logging.info(f\"\\nStart rule id got is {start_rule_id}\\n \")\n",
    "    while start_rule_id != \"END\":\n",
    "        # get the rules, next rule id to be evaluated\n",
    "        rule_to_evaluate, next_if_sucess, next_if_failure, stage, description, data_source = rule_id_mapping[str(start_rule_id)]  \n",
    "    \n",
    "        logging.info(f\"\\nInside the loop \\n rule_to_evaluate  {rule_to_evaluate}\\n \\\n",
    "                      \\nnext_if_sucess {next_if_sucess}\\n \\\n",
    "                      \\nnext_if_failure {next_if_failure}\\n \")\n",
    "        \n",
    "        # update the data_table if there is any change\n",
    "        case_id_data_tables = get_data_sources(tenant_id, case_id, 'case_id_based')\n",
    "        master_updated_tables = {} \n",
    "        if updated_tables:\n",
    "            master_updated_tables = get_data_sources(tenant_id, case_id, 'updated_tables')\n",
    "        # consolidate the data into data_tables\n",
    "        data_tables = {**case_id_data_tables, **master_data_tables, **master_updated_tables} \n",
    "        \n",
    "        # evaluate the rule\n",
    "        rules = [json.loads(rule_to_evaluate)] \n",
    "        BR  = BusinessRules(case_id, rules, data_tables)\n",
    "        decision = BR.evaluate_rule(rules[0])\n",
    "        \n",
    "        logging.info(f\"\\n got the decision {decision} for the rule id {start_rule_id}\")\n",
    "        logging.info(f\"\\n updates got are {BR.changed_fields}\")\n",
    "        \n",
    "        updates = {}\n",
    "        # update the updates if any\n",
    "        if BR.changed_fields:\n",
    "            updates = BR.changed_fields\n",
    "            update_tables(case_id, tenant_id, updates)\n",
    "\n",
    "        \n",
    "        # update the trace_data\n",
    "        trace_exec.append(start_rule_id)\n",
    "\n",
    "        logging.info(f\"\\n params data used from the rules are \\n {BR.params_data}\\n\")\n",
    "        # update the rule_params\n",
    "        trace_dict = {\n",
    "                        str(start_rule_id):{\n",
    "                            'description' : description if description else 'No description available in the database',\n",
    "                            'output' : \"\",\n",
    "                            'input' : to_DT_data(BR.params_data['input'])\n",
    "                            }\n",
    "                        }\n",
    "        rule_params.update(trace_dict)\n",
    "        # update the start_rule_id based on the decision\n",
    "        if decision:\n",
    "            start_rule_id = next_if_sucess\n",
    "        else:\n",
    "            start_rule_id = next_if_failure\n",
    "        logging.info(f\"\\n next rule id to execute is {start_rule_id}\\n\")\n",
    "        \n",
    "    \n",
    "    # off by one updates...\n",
    "    trace_exec.append(start_rule_id)\n",
    "    \n",
    "    # store the trace_exec and rule_params in the database\n",
    "    update_rule_params_query = f\"INSERT INTO `rule_data`(`id`, `case_id`, `rule_params`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `rule_params`=%s\"\n",
    "    params = [case_id, json.dumps(rule_params), json.dumps(rule_params)]\n",
    "    business_rules_db.execute(update_rule_params_query, params=params)\n",
    "    \n",
    "    update_trace_exec_query = f\"INSERT INTO `rule_data` (`id`, `case_id`, `trace_data`) VALUES ('NULL',%s,%s) ON DUPLICATE KEY UPDATE `trace_data`=%s\"\n",
    "    params = [case_id, json.dumps(trace_exec), json.dumps(trace_exec)]\n",
    "    business_rules_db.execute(update_trace_exec_query, params=params)\n",
    "    \n",
    "    logging.info(\"\\n Applied chained rules successfully\")\n",
    "    return 'Applied chained rules successfully'\n",
    "\n",
    "def run_group_rules(case_id, rules, data):\n",
    "    \"\"\"Run the rules\"\"\"\n",
    "    rules = [json.loads(rule) for rule in rules] \n",
    "    BR  = BusinessRules(case_id, rules, data)\n",
    "    updates = BR.evaluate_business_rules()\n",
    "    logging.info(f\"\\n updates from the group rules are \\n{updates}\\n\")\n",
    "    return updates\n",
    "\n",
    "def apply_business_rule(case_id, function_params, tenant_id, file_path):\n",
    "    \"\"\"Run the business rules based on the stage in function params and tenant_id\n",
    "    Args:\n",
    "        case_id: Unique id that we pass\n",
    "        function_params: Parameters that we get from the configurations\n",
    "        tenant_id: Tenant on which we have to apply the rules\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    updates = {} # keep a track of updates that are being made by business rules\n",
    "    try:\n",
    "        # get the stage from the function_parameters...As of now its first ele..\n",
    "        # need to make generic or key-value pairs\n",
    "        logging.info(f\"\\n case_id {case_id} \\nfunction_params {function_params} \\ntenant_id {tenant_id}\\n\")\n",
    "        stage = function_params['stage'][0]\n",
    "        \n",
    "        # no case id passed meaning the operations we are doing on the column wise not case_id wise .\n",
    "        # feature developed for karvy\n",
    "        column = False\n",
    "        if not case_id:\n",
    "            column = True\n",
    "            \n",
    "        # get the rules\n",
    "        rules = get_rules(tenant_id, stage)\n",
    "        \n",
    "        # get the mapping of the rules...basically a rule_id maps to a rule.\n",
    "        # useful for the chain rule evaluations\n",
    "        rule_id_mapping = {}\n",
    "        for ind, rule in rules.iterrows():\n",
    "            rule_id_mapping[rule['rule_id']] = [rule['rule_string'], rule['next_if_sucess'], rule['next_if_failure'], rule['stage'], rule['description'], rule['data_source']]\n",
    "\n",
    "        # if columwise processing then run those\n",
    "        if column:\n",
    "            output = run_chained_rules_column(file_path, rules)\n",
    "            return {'flag': True, 'message': 'Applied business rules columnwise successfully.'}\n",
    "\n",
    "            \n",
    "        # making it generic takes to take a type parameter from the database..\n",
    "        # As of now make it (all others  or chained) only\n",
    "        is_chain_rule = '' not in rule_id_mapping\n",
    "        \n",
    "        # get the required table data on which we will be applying business_rules  \n",
    "        case_id_data_tables = get_data_sources(tenant_id, case_id, 'case_id_based') \n",
    "        master_data_tables = get_data_sources(tenant_id, case_id, 'master', master=True)\n",
    "        \n",
    "        # consolidate the data into data_tables\n",
    "        data_tables = {**case_id_data_tables, **master_data_tables} \n",
    "        \n",
    "        logging.info(f\"\\ndata got from the tables is\\n\")\n",
    "        logging.info(data_tables)\n",
    "        \n",
    "        updates = {}\n",
    "        # apply business rules\n",
    "        if is_chain_rule:\n",
    "            run_chained_rules(case_id, tenant_id, rules)\n",
    "        else:\n",
    "            updates = run_group_rules(case_id, list(rules['rule_string']), data_tables)\n",
    "            \n",
    "        # update in the database, the changed fields eventually when all the stage rules were got\n",
    "        update_tables(case_id, tenant_id, updates)\n",
    "        \n",
    "        #  return the updates for viewing\n",
    "        return {'flag': True, 'message': 'Applied business rules successfully.', 'updates':updates}\n",
    "    except Exception as e:\n",
    "        logging.exception('Something went wrong while applying business rules. Check trace.')\n",
    "        return {'flag': False, 'message': 'Something went wrong while applying business rules. Check logs.', 'error':str(e)}\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     415
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %load BusinessRules.py\n",
    "import Lib\n",
    "import _StaticFunctions\n",
    "import _BooleanReturnFunctions\n",
    "import _AssignFunction\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# comment below two for local testing\n",
    "#from ace_logger import Logging\n",
    "#logging = Logging()\n",
    "\n",
    "# uncomment these below lines for local testing\n",
    "import logging \n",
    "logger=logging.getLogger() \n",
    "logger.setLevel(logging.ERROR) \n",
    "\n",
    "\n",
    "\n",
    "@Lib.add_methods_from(_StaticFunctions, _BooleanReturnFunctions, _AssignFunction) \n",
    "class BusinessRules():\n",
    "    \n",
    "    def __init__(self, case_id, rules, table_data, decision = False):\n",
    "        self.case_id = case_id\n",
    "        self.rules = rules\n",
    "        self.data_source = table_data\n",
    "        self.is_decision = decision\n",
    "\n",
    "        # fields which we are maintaining\n",
    "        self.changed_fields = {}\n",
    "        self.params_data = {}\n",
    "        self.params_data['input'] = []\n",
    "\n",
    "    def evaluate_business_rules(self):\n",
    "        \"\"\"Evaluate all the rules\"\"\"\n",
    "        for rule in self.rules:\n",
    "            logging.info(\"\\n Evaluating the rule: \" +f\"{rule} \\n\")\n",
    "            output = self.evaluate_rule(rule)\n",
    "            logging.info(\"\\n Output: \" +f\"{output} \\n\")\n",
    "        # update the changes fields in database\n",
    "        logging.info(f\"\\nchanged fields are \\n{self.changed_fields}\\n\")\n",
    "        return self.changed_fields\n",
    "    \n",
    "    def evaluate_rule(self,rule):\n",
    "        \"\"\"Evaluate the rule\"\"\"\n",
    "        logging.info(f\"\\nEvaluating the rule \\n{rule}\\n\")\n",
    "\n",
    "        rule_type = rule['rule_type']\n",
    "        \n",
    "        if  rule_type == 'static':\n",
    "            function_name = rule['function']\n",
    "            parameters = rule['parameters']\n",
    "            return  self.evaluate_static(function_name, parameters)\n",
    "    \n",
    "        if rule_type == 'condition':\n",
    "            evaluations = rule['evaluations']\n",
    "            return self.evaluate_condition(evaluations)\n",
    "    \n",
    "    def conditions_met(self, conditions):\n",
    "        \"\"\"Evaluate the conditions and give out the final decisoin\n",
    "        \n",
    "        \"\"\"\n",
    "        eval_string = ''\n",
    "        # return True if there are no conditions...that means we are doing else..\n",
    "        if not conditions:\n",
    "            return True\n",
    "        # evaluate the conditions\n",
    "        for condition in conditions:\n",
    "            logging.info(f\"Evaluting the condition {condition}\")\n",
    "            if condition == 'AND' or condition == 'OR':\n",
    "                eval_string += ' '+condition.lower()+' '\n",
    "            else:\n",
    "                eval_string += ' '+str(self.evaluate_rule(condition))+' '\n",
    "        logging.info(f\"\\n eval string is {eval_string} \\n output is {eval(eval_string)}\")\n",
    "        return eval(eval_string)\n",
    "\n",
    "    def evaluate_condition(self, evaluations):\n",
    "        \"\"\"Execute the conditional statements.\n",
    "\n",
    "        Args:\n",
    "            evaluations(dict) \n",
    "        Returns:\n",
    "            decision(boolean) If its is_decision.\n",
    "            True If conditions met and it is done with executions.\n",
    "            False For any other case (scenario).\n",
    "        \"\"\"\n",
    "        for each_if_conditions in evaluations:\n",
    "            conditions = each_if_conditions['conditions']\n",
    "            executions = each_if_conditions['executions']\n",
    "            logging.info(f'\\nconditions got are \\n{conditions}\\n')\n",
    "            logging.info(f'\\nexecutions got are \\n{executions}\\n')\n",
    "            decision = self.conditions_met(conditions)\n",
    "            \n",
    "            \"\"\"\n",
    "            Why this self.is_decision and decision ?\n",
    "                In decison tree there are only one set of conditions to check\n",
    "                But other condition rules might have (elif conditions which needs to be evaluate) \n",
    "            \"\"\"\n",
    "            if self.is_decision:\n",
    "                if decision:\n",
    "                    for rule in executions:\n",
    "                        self.evaluate_rule(rule)\n",
    "                logging.info(f\"\\n Decision got for the (for decision tree) condition\\n {decision}\")    \n",
    "                return decision\n",
    "            if decision:\n",
    "                for rule in executions:\n",
    "                    self.evaluate_rule(rule)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_param_value(self, param_object):\n",
    "        \"\"\"Returns the parameter value.\n",
    "\n",
    "        Args:\n",
    "            param_object(dict) The param dict from which we will parse and get the value.\n",
    "        Returns:\n",
    "            The value of the parameter\n",
    "        Note:\n",
    "            It returns a value of type we have defined. \n",
    "            If the parameter is itself a rule then it evaluates the rule and returns the value.\n",
    "        \"\"\"\n",
    "        logging.info(f\"\\nPARAM OBJECT IS {param_object}\\n\")\n",
    "        param_source = param_object['source']\n",
    "        if param_source == 'input_config':\n",
    "            table_key = param_object['table']\n",
    "            column_key = param_object['column']\n",
    "            table_key = table_key.strip() # strip for extra spaces\n",
    "            column_key = column_key.strip() # strip for extra spaces\n",
    "            logging.debug(f\"\\ntable is {table_key} and column key is {column_key}\\n\")\n",
    "            try:\n",
    "                data = {}\n",
    "                # update params data\n",
    "                data['type'] = 'from_table'\n",
    "                data['table'] = table_key\n",
    "                data['column'] = column_key\n",
    "                data['value'] = self.data_source[table_key][column_key]\n",
    "                self.params_data['input'].append(data)\n",
    "                return data['value']\n",
    "            except Exception as e:\n",
    "                logging.error(f\"\\ntable or column key not found\\n\")\n",
    "                logging.error(str(e))\n",
    "                logging.info(f\"\\ntable data is {self.data_source}\\n\")\n",
    "        if param_source == 'rule':\n",
    "            param_value = param_object['value']\n",
    "            return self.evaluate_rule(param_value)\n",
    "        if param_source == 'input':\n",
    "            param_value = param_object['value']\n",
    "#             param_value = str(param_value).strip() # converting into strings..need to check\n",
    "            return  param_value\n",
    "\n",
    "##############################################################################################################\n",
    "\n",
    "read_rule = {'rule_type':'static',\n",
    "              'function':'Read',\n",
    "              'parameters':{'path':'D:\\\\AlgonoX\\\\Python\\\\BUSINESS RULES\\\\KARVY_BusinessRules\\\\sample.csv'}\n",
    "}\n",
    "\n",
    "DR_CR_D_check = {'rule_type': 'static',\n",
    "        'function': 'CompareKeyValue',\n",
    "        'parameters': {'left_param':{'source': 'rule', 'value':read_rule},\n",
    "                       'operator':'==',\n",
    "                       'right_param':{'source':'input', 'value':'D'}\n",
    "                      }\n",
    "       }\n",
    "\n",
    "Amount_Debit = {'rule_type': 'static',\n",
    "                'function': 'Transform',\n",
    "                'parameters':[\n",
    "            {'param':{'source':'rule', 'value':read_rule}},\n",
    "            {'operator':'*'},\n",
    "            {'param':{'source':'input', 'value':\"-1\"}}\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "Amount_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'karvy_feed_copy', 'column':'Amount'}, \n",
    "                                                'assign_value':{'source':'rule', 'value':Amount_Debit}\n",
    "                      }\n",
    "}\n",
    "\n",
    "updating_Amount = {'rule_type': 'condition',\n",
    "                    'evaluations': [{ 'conditions':[DR_CR_D_check],'executions':[Amount_Assign_rule]\n",
    "\n",
    "        }]\n",
    "}\n",
    "\n",
    "\n",
    "########################################### Assign_Rules ###############################################################################\n",
    "\n",
    "Feed_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'karvy_feed_copy', 'column':'Feed'}, \n",
    "                                                'assign_value':{'source':'input', 'value':'CMS'}\n",
    "                      }\n",
    "}\n",
    "\n",
    "Sub_Feed_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'karvy_feed_copy', 'column':'Sub_Feed'}, \n",
    "                                                'assign_value':{'source':'input', 'value':'CMS'}\n",
    "                      }\n",
    "}\n",
    "\n",
    "ID_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'karvy_feed_copy', 'column':'ID'}, \n",
    "                                                'assign_value':{'source':'input', 'value':'0'}\n",
    "                      }\n",
    "}\n",
    "\n",
    "Feed_ID_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'karvy_feed_copy', 'column':'Feed_ID'}, \n",
    "                                                'assign_value':{'source':'input', 'value':''}\n",
    "                      }\n",
    "}\n",
    "\n",
    "########################################### Filtering_rule ###################################################################\n",
    "\n",
    "Code_check = {'rule_type': 'static',\n",
    "        'function': 'CompareKeyValue',\n",
    "        'parameters': {'left_param':{'source': 'input_config', 'table': 'karvy_feed_copy', 'column': 'Code'},\n",
    "                       'operator':'==',\n",
    "                       'right_param':{'source':'input', 'value':''}\n",
    "                      }\n",
    "       }\n",
    "\n",
    "Queue_Assign_rule = {'rule_type': 'static',\n",
    "                        'function': 'Assign',\n",
    "                        'parameters': {'assign_table':{'table':'Process_queue', 'column':'queue'}, \n",
    "                                                'assign_value':{'source':'input', 'value':'Maker'}\n",
    "                      }\n",
    "}    \n",
    "\n",
    "updating_Queue_Code = {'rule_type': 'condition',\n",
    "                    'evaluations': [{ 'conditions':[Code_check],'executions':[Queue_Assign_rule]\n",
    "\n",
    "        }]\n",
    "}\n",
    "\n",
    "########################################### Filtering_Date ###################################################################\n",
    "Filtering_Date = { 'rule_type': 'static',\n",
    "    'function'  : 'InCheckDate',\n",
    "    'parameters':{'start_date':{'source':'input', 'value':'2009-01-01'},\n",
    "                'end_date':{'source':'input', 'value':'2029-01-01'},\n",
    "                'date':{'source':'input_config','table': 'karvy_feed_copy', 'column': 'Date'}\n",
    "                }\n",
    "}\n",
    "\n",
    "###################################################################################################################################\n",
    "\n",
    "\n",
    "db_tables = {\n",
    "                    \"karvy_extraction\" : [\"karvy_feed_copy\"],\n",
    "                    #\"queues\":[\"process_queue\"]\n",
    "                }\n",
    "unique_id = 'Cms_10'\n",
    "\n",
    "###################################################################################################################################\n",
    "###################################### Based on Dataframe #########################################################################\n",
    "\"\"\"data = {\n",
    "    'NAME':['Rachel','Ross','joy','Monica','Phoebe'],\n",
    "    'ROLL':[497,498,499,500,501],\n",
    "    'AGE':[23,24,25,26,27]\n",
    "}\n",
    "\n",
    "data = pd.DataFrame(data)\"\"\"\n",
    "\n",
    "####################################################################################################################################\n",
    "count_df_rule = {'rule_type':'static',\n",
    "                  'function':'Contains',\n",
    "                  'parameters':{'table_name':'data','column_name':'NAME','value':{'source':'input','value':'Rachel'}}\n",
    "}\n",
    "\n",
    "assign_df_rule = {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'data', 'column':'ROLL'}, \n",
    "                                'assign_value':{'source':'input', 'value':'roll is'}\n",
    "}\n",
    "}\n",
    "#####################################################################################################################################\n",
    "############################################# Karvy_BusinessRules #########################################################\n",
    "#####################################################################################################################################\n",
    "\n",
    "read_rule = {'rule_type':'static',\n",
    "              'function':'Read',\n",
    "              'parameters':{'path':'D:\\\\AlgonoX\\\\Python\\\\BUSINESS RULES\\\\KARVY_BusinessRules\\\\sample.csv'}\n",
    "}\n",
    "\n",
    "#####################################################################################################################################\n",
    "filtering_rule = {'rule_type': 'static',\n",
    "                  'function': 'Filter',\n",
    "                  'parameters': {\n",
    "                  'from_table': {'source':'input','value':'ocr'},\n",
    "                 'lookup_filters':[\n",
    "                {\n",
    "                    'column_name': 'Amount',\n",
    "                    'compare_with':  {'source':'input', 'value':1000},\n",
    "                    'operator':'&'\n",
    "                },\n",
    "                {\n",
    "                    'column_name': 'Plan Code',\n",
    "                    'compare_with':  {'source':'input', 'value':'AG'},\n",
    "                    'operator':'|'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "}\n",
    "########################################## 05-10-2019 #######################################################################\n",
    "\n",
    "# ocr = pd.read_csv('D:\\\\AlgonoX\\\\Python\\\\BUSINESS RULES\\\\KARVY_BusinessRules\\\\sample.csv')\n",
    "\n",
    "amount_transform = {'rule_type': 'static',\n",
    "                'function': 'TransformDF',\n",
    "                'parameters':{\n",
    "                                'table':'ocr',\n",
    "                                'value1_colomn': {'source':'input', 'value': 'Amount'},\n",
    "                                'operator':'*',\n",
    "                                'value2': {'source':'input', 'value': -1}\n",
    "                            }\n",
    "}\n",
    "\n",
    "# ignore filter_test rule\n",
    "filter_test = {'rule_type': 'static',\n",
    "    'function': 'Filter',\n",
    "    'parameters': {\n",
    "            'from_table': 'ocr',\n",
    "            'lookup_filters':[\n",
    "                {\n",
    "                    'column_name': 'Amount',\n",
    "                    'lookup_operator' : '==',\n",
    "                    'compare_with':  {'source':'input', 'value': 10}\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "}\n",
    "\n",
    "where_test = {'rule_type': 'static',\n",
    "    'function': 'WhereClause',\n",
    "    'parameters': {\n",
    "            'data_frame':{'source':'rule', 'value': Amount_Debit},\n",
    "            'from_table': 'ocr',\n",
    "            'colomn':'Amount',\n",
    "            'lookup_filters':[\n",
    "                {\n",
    "                    'column_name': 'Plan Code',\n",
    "                    'lookup_operator' : '==',\n",
    "                    'compare_with':  {'source':'input', 'value': 'GP'}\n",
    "                }\n",
    "                \n",
    "            ]\n",
    "\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "read_rule = {'rule_type':'static',\n",
    "              'function':'Read',\n",
    "              'parameters':{\n",
    "                            'path':{'source':'input','value':'C:\\\\Users\\\\Algonox\\\\Desktop\\\\AlgonoxWork\\\\BusinessRulesModule\\\\run_business_rule\\\\sample.csv'},\n",
    "                           'table_name':{'source':'input','value':'ocr'}}\n",
    "}\n",
    "\n",
    "#####################################################################################################################################\n",
    "add_filetered_df_rule = {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'ocr', 'column':'Filtered'}, \n",
    "                                'assign_value':{'source':'input', 'value':'N'}\n",
    "                }\n",
    "             }\n",
    "\n",
    "#################################################################################################################################\n",
    "add_matched_assign_df_rule = {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'ocr', 'column':'Matched'}, \n",
    "                                'assign_value':{'source':'input', 'value':'0'}\n",
    "                }\n",
    "             }\n",
    "########################################################################################################################\n",
    "add_dr_cr_assign_df_rule = {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'ocr', 'column':'DR_CR'}, \n",
    "                                'assign_value':{'source':'input', 'value':'D'}\n",
    "                }\n",
    "             }\n",
    "#######################################################################################################################\n",
    "add_amount = {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'ocr', 'column':'Amount'}, \n",
    "                                'assign_value':{'source':'input', 'value':50}\n",
    "                }\n",
    "             }\n",
    "#######################################################################################################################\n",
    "\n",
    "amount_transform = {'rule_type': 'static',\n",
    "                'function': 'TransformDF',\n",
    "                'parameters':{\n",
    "                                'table':'ocr',\n",
    "                                'value1_colomn': {'source':'input', 'value': 'Amount'},\n",
    "                                'operator':'*',\n",
    "                                'value2': {'source':'input', 'value': -1}\n",
    "                            }\n",
    "}\n",
    "\n",
    "# assign_amoutn_transform =  {'rule_type':'static',\n",
    "#                   'function':'Assign',\n",
    "#                   'parameters':{'assign_table':{'table':'ocr', 'column':'Amount'}, \n",
    "#                                 'assign_value':{'source':'rule', 'value':amount_transform}\n",
    "#                 }\n",
    "#              }\n",
    "\n",
    "where_rule = {'rule_type': 'static',\n",
    "    'function': 'WhereClause',\n",
    "    'parameters': {\n",
    "            'data_frame':{'source':'rule', 'value': amount_transform},\n",
    "            'from_table': 'ocr',\n",
    "            'column':'Amount',\n",
    "            'lookup_filters':[\n",
    "                {\n",
    "                    'column_name': 'DR_CR',\n",
    "                    'lookup_operator' : '==',\n",
    "                    'compare_with':  {'source':'input', 'value': 'D'}\n",
    "                }\n",
    "                \n",
    "            ]\n",
    "\n",
    "        }\n",
    "}\n",
    "\n",
    "\n",
    "assign_where_transform =  {'rule_type':'static',\n",
    "                  'function':'Assign',\n",
    "                  'parameters':{'assign_table':{'table':'ocr', 'column':'Amount'}, \n",
    "                                'assign_value':{'source':'rule', 'value':where_rule}\n",
    "                }\n",
    "             }\n",
    "\n",
    "rules = [read_rule, add_amount, add_filetered_df_rule, add_matched_assign_df_rule, add_dr_cr_assign_df_rule, assign_where_transform]\n",
    "rules= [read_rule]\n",
    "a = BusinessRules('1234',rules,{},decision = True)\n",
    "(a.evaluate_business_rules())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def writeToCsv(df, file_path, required_standard_mapping):\n",
    "    \"\"\"Write the dataframe to csv\"\"\"\n",
    "    try:\n",
    "\n",
    "        # first find whether any csv exists with the file_name\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "\n",
    "        mode = 'a'\n",
    "        headers = False\n",
    "        df.to_csv(file_path, columns=required_columns, mode=mode, header=headers, index=False)\n",
    "    except:\n",
    "        # no sample file exists\n",
    "        headers = True\n",
    "        mode = 'w'\n",
    "        \n",
    "        # rename the columns in the dataframe to standardized columns            \n",
    "        df = df.rename(columns=required_standard_mapping)\n",
    "        \n",
    "        required_columns = required_standard_mapping.values()\n",
    "\n",
    "        df.to_csv(file_path, columns=required_columns, mode=mode, header=headers, index=False)\n",
    "                \n",
    "    return \"Written to csv successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Algonox\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (42,48,51,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sample = pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sample.columns)\n",
    "s_req_ = {'Investor type' : 'TYPE'}\n",
    "file_path = 'C:\\\\Users\\\\Algonox\\\\Desktop\\\\sample_small.csv'\n",
    "df = sample.head(5)\n",
    "writeToCsv(df, file_path, s_req_)\n",
    "# df.re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(5).to_csv('C:\\\\Users\\\\Algonox\\\\Desktop\\\\sample_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,3]\n",
    "\n",
    "c = {e:f for (e,f) in zip(a,b)}\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(5)\n",
    "sample.head(5).to_csv('C:\\\\Users\\\\Algonox\\\\Desktop\\\\sample_small_.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.data_source['ocr'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['Investor type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6f07360337a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"12345\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = -2\n",
    "end = None\n",
    "a[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Algonox\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "sample['Amount'].iloc[0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            NaN\n",
       "1        -150000\n",
       "2           -500\n",
       "3          -2000\n",
       "4          -2000\n",
       "5         -40000\n",
       "6         -39000\n",
       "7         -50000\n",
       "8         -50000\n",
       "9        -190000\n",
       "10         -1000\n",
       "11         -1000\n",
       "12       -170000\n",
       "13        -1e+06\n",
       "14         -2500\n",
       "15        -25000\n",
       "16        -50000\n",
       "17         -1500\n",
       "18         -3000\n",
       "19         -4000\n",
       "20         -1500\n",
       "21         -5000\n",
       "22         -1000\n",
       "23         -1000\n",
       "24        -15000\n",
       "25        -15000\n",
       "26         -1550\n",
       "27         -1000\n",
       "28         -5000\n",
       "29         -1000\n",
       "           ...  \n",
       "166701     -1000\n",
       "166702     -2000\n",
       "166703    -25000\n",
       "166704     -2000\n",
       "166705     -5000\n",
       "166706     -2000\n",
       "166707    -25000\n",
       "166708     -2000\n",
       "166709     -2000\n",
       "166710     -5000\n",
       "166711     -5000\n",
       "166712     -5000\n",
       "166713     -2000\n",
       "166714     -5900\n",
       "166715     -5000\n",
       "166716    -14000\n",
       "166717     -2000\n",
       "166718    -50000\n",
       "166719     -2000\n",
       "166720     -2900\n",
       "166721    -25000\n",
       "166722     -2000\n",
       "166723     -2000\n",
       "166724     -2000\n",
       "166725     -2000\n",
       "166726     -2000\n",
       "166727      -100\n",
       "166728    -20000\n",
       "166729     -2000\n",
       "166730    -50000\n",
       "Name: Amount, Length: 166731, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['Amount']*-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
